{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "253770a5",
   "metadata": {},
   "source": [
    "# Word Embedding Translator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e015f1b",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7230eb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "import openpyxl\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances, manhattan_distances\n",
    "\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36ae151",
   "metadata": {},
   "source": [
    "## 1 - Loading data\n",
    "Loading the models and sentences used.\n",
    "- Models: https://fasttext.cc/docs/en/crawl-vectors.html\n",
    "- Sentences: https://github.com/alexa/massive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f117e715",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(model_path, sentences_path, limit = None):\n",
    "    '''\n",
    "    Load models from FastText folder and sentences from Amazon Massive folder.\n",
    "    \n",
    "    Params:\n",
    "    - model_path: path to the folder containing all models used, i.e., FastText\n",
    "    - sentences_path: path to the folder containing all sentences used, i.e., Amazon_Massive\n",
    "    - limit: define a limit in case your have low computer power, e.g., 5000\n",
    "    \n",
    "    Return:\n",
    "    Tuple containing the language model and its corresponding sentences\n",
    "    '''\n",
    "\n",
    "    model = KeyedVectors.load_word2vec_format(model_path, unicode_errors = 'replace', limit = limit)\n",
    "    sentences = pd.read_json(sentences_path, lines = True)['utt']\n",
    "    \n",
    "    return model, sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab38a6d",
   "metadata": {},
   "source": [
    "Defining data path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4f7b818",
   "metadata": {},
   "outputs": [],
   "source": [
    "FASTTEXT_PATH = 'Datasets/FastText/'\n",
    "MASSIVE_PATH = 'Datasets/Amazon_Massive/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "553117db",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATHS = {\n",
    "    'en': [ FASTTEXT_PATH + 'cc.en.300.vec', MASSIVE_PATH + 'en-US.jsonl' ],\n",
    "    'pt': [ FASTTEXT_PATH + 'cc.pt.300.vec', MASSIVE_PATH + 'pt-PT.jsonl' ],\n",
    "    'es': [ FASTTEXT_PATH + 'cc.es.300.vec', MASSIVE_PATH + 'es-ES.jsonl' ],\n",
    "    'fr': [ FASTTEXT_PATH + 'cc.fr.300.vec', MASSIVE_PATH + 'fr-FR.jsonl' ],\n",
    "    'it': [ FASTTEXT_PATH + 'cc.it.300.vec', MASSIVE_PATH + 'it-IT.jsonl' ],\n",
    "    'ro': [ FASTTEXT_PATH + 'cc.ro.300.vec', MASSIVE_PATH + 'ro-RO.jsonl' ],\n",
    "    'de': [ FASTTEXT_PATH + 'cc.de.300.vec', MASSIVE_PATH + 'de-DE.jsonl' ],\n",
    "    'da': [ FASTTEXT_PATH + 'cc.da.300.vec', MASSIVE_PATH + 'da-DK.jsonl' ],\n",
    "    'nl': [ FASTTEXT_PATH + 'cc.nl.300.vec', MASSIVE_PATH + 'nl-NL.jsonl' ],\n",
    "    'sv': [ FASTTEXT_PATH + 'cc.sv.300.vec', MASSIVE_PATH + 'sv-SE.jsonl' ],\n",
    "}\n",
    "\n",
    "LANGUAGES = PATHS.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c564e517",
   "metadata": {},
   "source": [
    "**Note**: the cell below takes approximately 5 to 6 minutes per model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08bf0f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Datasets/FastTextcc.en.300.vec\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Datasets/FastTextcc.en.300.vec'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\User\\Desktop\\WordEmbedding-Translator\\Translator.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/WordEmbedding-Translator/Translator.ipynb#ch0000009?line=4'>5</a>\u001b[0m     sentences \u001b[39m=\u001b[39m value[\u001b[39m1\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/WordEmbedding-Translator/Translator.ipynb#ch0000009?line=6'>7</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLoading\u001b[39m\u001b[39m\"\u001b[39m, model)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/WordEmbedding-Translator/Translator.ipynb#ch0000009?line=7'>8</a>\u001b[0m     MODELS[language], SENTENCES[language] \u001b[39m=\u001b[39m load_files(model, sentences)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/WordEmbedding-Translator/Translator.ipynb#ch0000009?line=8'>9</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFinished loading\u001b[39m\u001b[39m\"\u001b[39m, model)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/WordEmbedding-Translator/Translator.ipynb#ch0000009?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mAll models and sentences are now loaded!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\User\\Desktop\\WordEmbedding-Translator\\Translator.ipynb Cell 5'\u001b[0m in \u001b[0;36mload_files\u001b[1;34m(model_path, sentences_path, limit)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/WordEmbedding-Translator/Translator.ipynb#ch0000004?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_files\u001b[39m(model_path, sentences_path, limit \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/WordEmbedding-Translator/Translator.ipynb#ch0000004?line=1'>2</a>\u001b[0m     \u001b[39m'''\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/WordEmbedding-Translator/Translator.ipynb#ch0000004?line=2'>3</a>\u001b[0m \u001b[39m    Load models from FastText folder and sentences from Amazon Massive folder.\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/WordEmbedding-Translator/Translator.ipynb#ch0000004?line=3'>4</a>\u001b[0m \u001b[39m    \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/WordEmbedding-Translator/Translator.ipynb#ch0000004?line=10'>11</a>\u001b[0m \u001b[39m    Tuple containing the language model and its corresponding sentences\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/WordEmbedding-Translator/Translator.ipynb#ch0000004?line=11'>12</a>\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/WordEmbedding-Translator/Translator.ipynb#ch0000004?line=13'>14</a>\u001b[0m     model \u001b[39m=\u001b[39m KeyedVectors\u001b[39m.\u001b[39;49mload_word2vec_format(model_path, unicode_errors \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mreplace\u001b[39;49m\u001b[39m'\u001b[39;49m, limit \u001b[39m=\u001b[39;49m limit)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/WordEmbedding-Translator/Translator.ipynb#ch0000004?line=14'>15</a>\u001b[0m     sentences \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_json(sentences_path, lines \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)[\u001b[39m'\u001b[39m\u001b[39mutt\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/WordEmbedding-Translator/Translator.ipynb#ch0000004?line=16'>17</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m model, sentences\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gensim\\models\\keyedvectors.py:1723\u001b[0m, in \u001b[0;36mKeyedVectors.load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[0;32m   1676\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m   1677\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_word2vec_format\u001b[39m(\n\u001b[0;32m   1678\u001b[0m         \u001b[39mcls\u001b[39m, fname, fvocab\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, binary\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf8\u001b[39m\u001b[39m'\u001b[39m, unicode_errors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstrict\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1679\u001b[0m         limit\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, datatype\u001b[39m=\u001b[39mREAL, no_header\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   1680\u001b[0m     ):\n\u001b[0;32m   1681\u001b[0m     \u001b[39m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[0;32m   1682\u001b[0m \n\u001b[0;32m   1683\u001b[0m \u001b[39m    Warnings\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1721\u001b[0m \n\u001b[0;32m   1722\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1723\u001b[0m     \u001b[39mreturn\u001b[39;00m _load_word2vec_format(\n\u001b[0;32m   1724\u001b[0m         \u001b[39mcls\u001b[39;49m, fname, fvocab\u001b[39m=\u001b[39;49mfvocab, binary\u001b[39m=\u001b[39;49mbinary, encoding\u001b[39m=\u001b[39;49mencoding, unicode_errors\u001b[39m=\u001b[39;49municode_errors,\n\u001b[0;32m   1725\u001b[0m         limit\u001b[39m=\u001b[39;49mlimit, datatype\u001b[39m=\u001b[39;49mdatatype, no_header\u001b[39m=\u001b[39;49mno_header,\n\u001b[0;32m   1726\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gensim\\models\\keyedvectors.py:2052\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[0;32m   2049\u001b[0m             counts[word] \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(count)\n\u001b[0;32m   2051\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mloading projection weights from \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, fname)\n\u001b[1;32m-> 2052\u001b[0m \u001b[39mwith\u001b[39;00m utils\u001b[39m.\u001b[39;49mopen(fname, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m fin:\n\u001b[0;32m   2053\u001b[0m     \u001b[39mif\u001b[39;00m no_header:\n\u001b[0;32m   2054\u001b[0m         \u001b[39m# deduce both vocab_size & vector_size from 1st pass over file\u001b[39;00m\n\u001b[0;32m   2055\u001b[0m         \u001b[39mif\u001b[39;00m binary:\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\smart_open\\smart_open_lib.py:188\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, compression, transport_params)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[39mif\u001b[39;00m transport_params \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     transport_params \u001b[39m=\u001b[39m {}\n\u001b[1;32m--> 188\u001b[0m fobj \u001b[39m=\u001b[39m _shortcut_open(\n\u001b[0;32m    189\u001b[0m     uri,\n\u001b[0;32m    190\u001b[0m     mode,\n\u001b[0;32m    191\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[0;32m    192\u001b[0m     buffering\u001b[39m=\u001b[39;49mbuffering,\n\u001b[0;32m    193\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[0;32m    194\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    195\u001b[0m     newline\u001b[39m=\u001b[39;49mnewline,\n\u001b[0;32m    196\u001b[0m )\n\u001b[0;32m    197\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m     \u001b[39mreturn\u001b[39;00m fobj\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\smart_open\\smart_open_lib.py:361\u001b[0m, in \u001b[0;36m_shortcut_open\u001b[1;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[39mif\u001b[39;00m errors \u001b[39mand\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m    359\u001b[0m     open_kwargs[\u001b[39m'\u001b[39m\u001b[39merrors\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m errors\n\u001b[1;32m--> 361\u001b[0m \u001b[39mreturn\u001b[39;00m _builtin_open(local_path, mode, buffering\u001b[39m=\u001b[39mbuffering, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mopen_kwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Datasets/FastTextcc.en.300.vec'"
     ]
    }
   ],
   "source": [
    "MODELS, SENTENCES = {}, {}\n",
    "\n",
    "for language, value in PATHS.items():\n",
    "    model = value[0]\n",
    "    sentences = value[1]\n",
    "\n",
    "    print(\"Loading\", model)\n",
    "    MODELS[language], SENTENCES[language] = load_files(model, sentences)\n",
    "    print(\"Finished loading\", model)\n",
    "\n",
    "print(\"\\nAll models and sentences are now loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e14fb99",
   "metadata": {},
   "source": [
    "## 2 - Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bceeb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLES = { key: [] for key in LANGUAGES }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67d11b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since all sentence files have the same length, we chose one at random for the range function.\n",
    "# We prove this in the cell below\n",
    "for idx in range(len(SENTENCES['en'])):\n",
    "    \n",
    "    actual_sentence = { key: [] for key in LANGUAGES }\n",
    "    \n",
    "    try:\n",
    "        for lang, sent in SENTENCES.items():\n",
    "            for word in sent[idx].split(' '):\n",
    "                actual_sentence[lang].append(MODELS[lang][word])\n",
    "\n",
    "    except KeyError:\n",
    "        continue\n",
    "    \n",
    "    for key, value in actual_sentence.items():\n",
    "        SAMPLES[key].append([SENTENCES[key][idx], sum(value)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d43f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in SENTENCES:\n",
    "    SIZE_SAMPLES = len(SAMPLES[key])\n",
    "    print(f'Total sentences in { key } file: { len(SENTENCES[key]) } -> Model { key } samples: { len(SAMPLES[key]) } ({ len(SAMPLES[key]) / len(SENTENCES[key]) * 100:.2f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aefab70",
   "metadata": {},
   "source": [
    "Splitting into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fac669b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_RATE = int(SIZE_SAMPLES * 0.7)\n",
    "\n",
    "TRAIN_SET = { key: SAMPLES[key][:SPLIT_RATE] for key in LANGUAGES }\n",
    "TEST_SET = { key: SAMPLES[key][SPLIT_RATE:] for key in LANGUAGES }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf7d5e4",
   "metadata": {},
   "source": [
    "## 3 - Translating words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1711e3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSLATIONS = { key: { lang: None for lang in LANGUAGES if lang != key } for key in LANGUAGES }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5bc3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "for origin, target in it.permutations(LANGUAGES, 2): \n",
    "\n",
    "    samples_origin = [sample[1] for sample in TRAIN_SET[origin]]\n",
    "    samples_target = [sample[1] for sample in TRAIN_SET[target]]\n",
    "\n",
    "    U, Sig, Vt = np.linalg.svd(np.transpose(samples_origin) @ samples_target)\n",
    "    \n",
    "    TRANSLATOR = np.transpose(Vt) @ np.transpose(U)\n",
    "    TRANSLATIONS[origin][target] = TRANSLATOR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4325e13",
   "metadata": {},
   "source": [
    "### List of examples words\n",
    "**Note**: only single words can be written, i.e., compound words like \"washing machine\" will result in Error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aed3f9c",
   "metadata": {},
   "source": [
    "- English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267b75aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_WORD_LIST = [\n",
    "    'specification',\n",
    "    'book',\n",
    "    'duckling',\n",
    "    'machine',\n",
    "    'headphones'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04930bcf",
   "metadata": {},
   "source": [
    "- Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f14161",
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_WORD_LIST = [\n",
    "    'hola',\n",
    "    'sí',\n",
    "    'computadora',\n",
    "    'país'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9107823",
   "metadata": {},
   "source": [
    "- Portuguese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed70d7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "PT_WORD_LIST = [\n",
    "    'sapato',\n",
    "    'flor',\n",
    "    'aniversário',\n",
    "    'saudades',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad3f141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(word_list, origin_lang, target_lang):\n",
    "    '''\n",
    "    Function to translate one word from one language to another.\n",
    "\n",
    "    Params:\n",
    "    - word_list: list of example words.\n",
    "    - origin_lang: language in which the words in word_list are written\n",
    "    - target_lang: language you wish to know the translation\n",
    "\n",
    "    Example of usage:\n",
    "    translate(PT_WORD_LIST, 'es', 'pt')\n",
    "    '''\n",
    "    \n",
    "    for word in word_list:\n",
    "        print(\"Original word:\", word)\n",
    "        print(\"Top 10 most similar words in\", target_lang)\n",
    "        print(MODELS[target_lang].most_similar(TRANSLATIONS[origin_lang][target_lang] @ MODELS[origin_lang][word]))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18fa53d",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ff3272",
   "metadata": {},
   "source": [
    "- Portuguese -> Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c2d369",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "translate(PT_WORD_LIST, 'pt', 'es')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29a05a8",
   "metadata": {},
   "source": [
    "- Portuguese -> English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0777db3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(PT_WORD_LIST, 'pt', 'en')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f82e71",
   "metadata": {},
   "source": [
    "- Spanish -> English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c980d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(ES_WORD_LIST, 'es', 'en')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6b2270",
   "metadata": {},
   "source": [
    "- English -> Portuguese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3dc7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(EN_WORD_LIST, 'en', 'pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246e534a",
   "metadata": {},
   "source": [
    "## 4 - Translating words using intermediate languages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba21e7c",
   "metadata": {},
   "source": [
    "### Getting the most similar word in each language it pass.\n",
    "Most expensive (uses most_similar multiple times) and try to aproximate a word each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0596b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intermediate_most_similar_word(word_list, origin_lang, intermediate_lang, target_lang):\n",
    "    '''\n",
    "    Translate one word from one language to another passing by an intermediate language.\n",
    "    In this function, we use the result of the most similar word of the intermediate language to make the next translation.\n",
    "\n",
    "    Params:\n",
    "    - word_list: list of example words.\n",
    "    - origin_lang: language in which the words in word_list are written\n",
    "    - intermediate_lang: intermediate language which translation between origin_lang and target_lang passes by\n",
    "    - target_lang: language you wish to know the translation\n",
    "\n",
    "    Example of usage:\n",
    "    intermediate_most_similar_word(PT_WORD_LIST, 'es', 'pt', 'en')\n",
    "    '''\n",
    "    \n",
    "    for word in word_list:\n",
    "        print(\"Original word:\", word)\n",
    "        \n",
    "        intermediate_word = MODELS[intermediate_lang].most_similar(TRANSLATIONS[origin_lang][intermediate_lang] @ MODELS[origin_lang][word])[0][0]\n",
    "        print(\"Most similar word according to intermediate language:\", intermediate_word)\n",
    "\n",
    "        translated_language = MODELS[target_lang].most_similar(TRANSLATIONS[intermediate_lang][target_lang] @ MODELS[intermediate_lang][intermediate_word])\n",
    "        print(\"Top 10 most similar words in target language passing by the intermediate language:\")\n",
    "        print(translated_language)\n",
    "        \n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5f3c6e",
   "metadata": {},
   "source": [
    "- Portuguese -> English -> Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c8b0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_most_similar_word(PT_WORD_LIST, 'pt', 'en', 'es')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74244ca1",
   "metadata": {},
   "source": [
    "- Spanish -> Portuguese -> English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0073786",
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_most_similar_word(ES_WORD_LIST, 'es', 'pt', 'en')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9842db97",
   "metadata": {},
   "source": [
    "- English -> Spanish -> Portuguese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc536b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_most_similar_word(EN_WORD_LIST, 'en', 'es', 'pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340e25f7",
   "metadata": {},
   "source": [
    "### Using the vector transformed to each subspace.\n",
    "Uses most_similar and try to approximate the word just one time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd051d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intermediate_most_similar_vector(word_list, origin_lang, intermediate_lang, target_lang):\n",
    "    '''\n",
    "    Translate one word from one language to another passing by an intermediate language.\n",
    "    In this function, we use the result of the vector of the translation passing by the intermediate language to make the next translation.\n",
    "\n",
    "    Params:\n",
    "    - word_list: list of example words.\n",
    "    - origin_lang: language in which the words in word_list are written\n",
    "    - intermediate_lang: intermediate language which translation between origin_lang and target_lang passes by\n",
    "    - target_lang: language you wish to know the translation\n",
    "\n",
    "    Example of usage:\n",
    "    intermediate_most_similar_vector(PT_WORD_LIST, 'es', 'pt', 'en')\n",
    "    '''\n",
    "    \n",
    "    for word in word_list:\n",
    "        print(\"Original word:\", word)\n",
    "\n",
    "        intermediate_vector = TRANSLATIONS[origin_lang][intermediate_lang] @ MODELS[origin_lang][word]\n",
    "        translated_vector = MODELS[target_lang].most_similar(TRANSLATIONS[intermediate_lang][target_lang] @ intermediate_vector)\n",
    "        \n",
    "        print(\"Top 10 most similar words in target language passing by the intermediate language:\")\n",
    "        print(translated_vector)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a173201",
   "metadata": {},
   "source": [
    "- Portuguese -> English -> Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161a58ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_most_similar_vector(PT_WORD_LIST, 'pt', 'en', 'es')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc0af35",
   "metadata": {},
   "source": [
    "- Spanish -> Portuguese -> English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2788b6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_most_similar_vector(ES_WORD_LIST, 'es', 'pt', 'en')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e0fd3f",
   "metadata": {},
   "source": [
    "- English -> Spanish -> Portuguese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9859e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_most_similar_vector(EN_WORD_LIST, 'en', 'es', 'pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f3cca3",
   "metadata": {},
   "source": [
    "## 5 - Evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460e30dd",
   "metadata": {},
   "source": [
    "Theorically speaking, translating the vector that one sentence represents to another should result in a similar sentence. For that purpose, we evaluate our results using the cosine similarity, which range is from -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78446c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_single_cosine_similarity(origin_lang, target_lang):\n",
    "    '''\n",
    "    Evaluate cosine similarity between single sentences.\n",
    "    Cosine similarity has an interval from -1 to 1, and the closer to 1 the value is, more similar the params are.\n",
    "\n",
    "    Params:\n",
    "    - origin_lang: language in which the words in word_list are written\n",
    "    - target_lang: language you wish to know the translation\n",
    "\n",
    "    Example of usage:\n",
    "    evaluate_single_cosine_similarity('pt', 'en')\n",
    "    '''\n",
    "    \n",
    "    for index in range(5):\n",
    "        print(TEST_SET[origin_lang][index][0], '->', TEST_SET[target_lang][index][0])\n",
    "\n",
    "        vector_translated = TRANSLATIONS[origin_lang][target_lang] @ TEST_SET[origin_lang][index][1]\n",
    "        vector_target = TEST_SET[target_lang][index][1]\n",
    "\n",
    "        print(\"Cossine similarity:\", cosine_similarity([vector_translated], [vector_target])[0][0], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb53fcd",
   "metadata": {},
   "source": [
    "- Portuguese -> English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7457740",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_single_cosine_similarity('pt', 'en')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac78c26e",
   "metadata": {},
   "source": [
    "- Portuguese -> Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e394f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_single_cosine_similarity('pt', 'es')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a56e65",
   "metadata": {},
   "source": [
    "- English -> Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf664a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_single_cosine_similarity('en', 'es')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ad78e8",
   "metadata": {},
   "source": [
    "### Avaliating path\n",
    "We use the following metrics for that purpose:\n",
    "- Cosine similarity\n",
    "- Euclidean distance\n",
    "- Manhattan distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc908230",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise(iterable):\n",
    "    '''\n",
    "    Return successive overlapping pairs taken from the input iterable.\n",
    "    The number of 2-tuples in the output iterator will be one fewer than the number of inputs. \n",
    "    It will be empty if the input iterable has fewer than two values.\n",
    "    pairwise('ABCDEFG') --> AB BC CD DE EF FG\n",
    "\n",
    "    Source: https://docs.python.org/3/library/itertools.html#itertools.pairwise\n",
    "    '''\n",
    "    a, b = it.tee(iterable)\n",
    "    next(b, None)\n",
    "    return zip(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426214bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avaliate_path(path):\n",
    "    '''\n",
    "    Avaliate the translation path using cosine similarity, euclidean distance and manhattan distance.\n",
    "\n",
    "    Params:\n",
    "    - path: path of desired translation\n",
    "    \n",
    "    Return:\n",
    "    - Score of each avaliation method\n",
    "    \n",
    "    Example of usage:\n",
    "    avaliate_path(['pt', 'en', 'es'])\n",
    "    '''\n",
    "    \n",
    "    #return (1, 1, 1) #using for test only\n",
    "    translation_matrix = np.identity(300)\n",
    "\n",
    "    for (origin, target) in pairwise(path):\n",
    "        translation_matrix = TRANSLATIONS[origin][target] @ translation_matrix\n",
    "    \n",
    "    vectors = [translation_matrix @ v for _, v in TEST_SET[path[0]] ]\n",
    "    vectors_target = [v for _, v in TEST_SET[path[-1]]]\n",
    "    \n",
    "    mean_cos_sim = sum([cosine_similarity([v1], [v2]) for v1, v2 in zip(vectors, vectors_target)])/ len(vectors)\n",
    "    mean_euc_dist = sum([euclidean_distances([v1], [v2]) for v1, v2 in zip(vectors, vectors_target)])/ len(vectors)\n",
    "    mean_man_dist = sum([manhattan_distances([v1], [v2]) for v1, v2 in zip(vectors, vectors_target)])/ len(vectors)\n",
    "    \n",
    "    return mean_cos_sim, mean_euc_dist, mean_man_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4005bbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avaliate_possible_paths(languages):\n",
    "    '''\n",
    "    Avaliate possible paths from first to last language on the list, changing the languages in the middle\n",
    "    \n",
    "    Params:\n",
    "    - languages: list of languages\n",
    "    \n",
    "    Return:\n",
    "    - Dataframe with the score of each path on cossine similarity, euclidean distance and manhattan distance\n",
    "    \n",
    "    Example of usage:\n",
    "    avaliate_paths(['pt', 'en', 'es'])\n",
    "    '''\n",
    "    \n",
    "    start = languages[0]\n",
    "    end = languages[-1]\n",
    "    \n",
    "    paths = [[start, end]]\n",
    "    for i in range(len(languages) -2 ):\n",
    "        for comb in it.combinations(languages[1:-1], i+1):\n",
    "            paths.append([start] + list(comb) + [end])\n",
    "    \n",
    "    scores = [avaliate_path(p) for p in paths]\n",
    "    index = [ ' -> '.join(p) for p in paths]\n",
    "    return pd.DataFrame(data=scores, columns=['Cosine Similarity', 'Euclidean Distance', 'Manhattan Distance'], index=index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866007a0",
   "metadata": {},
   "source": [
    "## 6 - Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cdbe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename = 'Images/proto-indo-european.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2c8082",
   "metadata": {},
   "source": [
    "### Experiment #1: Portuguese - English - Spanish\n",
    "In this experiment, we intend to evaluate how good is a translation between two languages from the Latin group, such as Portuguese and Spanish, and if adding a language from an outer group, such as English from the West Germanic, affects the quality of the translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b908b884",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1 = avaliate_possible_paths( ['pt', 'en', 'pt', 'en', 'es'])\n",
    "exp1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf2bb3d",
   "metadata": {},
   "source": [
    "### Experiment #2: Portuguese - Spanish - French - Italian - Romanian\n",
    "In this experiment, we intend to evaluate translations between multiple languages from the same Latin group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfa460b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = avaliate_possible_paths(['pt', 'fr', 'it', 'ro', 'es'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169d14de",
   "metadata": {},
   "source": [
    "> Adding a test with English in the middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b4164d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = ['pt', 'fr', 'it', 'en', 'ro', 'es']\n",
    "df2 = pd.DataFrame(data=[avaliate_path(path)], index=[' -> '.join(path)], columns=df.columns)\n",
    "exp2 = pd.concat([df, df2])\n",
    "exp2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e11951d",
   "metadata": {},
   "source": [
    "### Experiment #3: English - German - Swedish - Dutch - Danish\n",
    "In this experiment, we intend to evaluate translations between multiple languages from the same Germanic group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77458df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = avaliate_possible_paths(['en', 'sv', 'nl', 'da', 'de'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011f0d13",
   "metadata": {},
   "source": [
    "> Adding a test with Italian in the middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e4220e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = ['en', 'sv', 'it', 'nl', 'da', 'de']\n",
    "df2 = pd.DataFrame(data=[avaliate_path(path)], index=[' -> '.join(path)], columns=df.columns)\n",
    "exp3 = pd.concat([df, df2])\n",
    "exp3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4304f41",
   "metadata": {},
   "source": [
    "### Experiment #4: Portuguese - German - French - English - Spanish\n",
    "In this experiment, we intend to evaluate translations using languages from different groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddc0304",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp4 = avaliate_possible_paths(['pt', 'de', 'fr', 'en', 'es'])\n",
    "exp4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47926541",
   "metadata": {},
   "source": [
    "### Experiment #5: English - Spanish - Swedish - Italian - German\n",
    "In this experiment, we intend to evaluate translations using languages from different groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d72124",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp5 = avaliate_possible_paths(['en', 'es', 'sv', 'it', 'de'])\n",
    "exp5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09768a36",
   "metadata": {},
   "source": [
    "### Saving results to spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149a99ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'results.xlsx'\n",
    "with pd.ExcelWriter(path) as writer:\n",
    "    #writer.book = openpyxl.load_workbook(path)\n",
    "    exp1.to_excel(writer, sheet_name='Experiment 1')\n",
    "    exp2.to_excel(writer, sheet_name='Experiment 2')\n",
    "    exp3.to_excel(writer, sheet_name='Experiment 3')\n",
    "    exp4.to_excel(writer, sheet_name='Experiment 4')\n",
    "    exp5.to_excel(writer, sheet_name='Experiment 5')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cf92aa13fedf815d5c8dd192b8d835913fde3e8bc926b2a0ad6cc74ef2ba3ca2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
