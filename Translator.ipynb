{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "253770a5",
   "metadata": {},
   "source": [
    "# Word Embedding Translator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e015f1b",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7230eb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36ae151",
   "metadata": {},
   "source": [
    "## 1 - Loading data\n",
    "Loading the models and sentences used.\n",
    "- Models: https://fasttext.cc/docs/en/crawl-vectors.html\n",
    "- Sentences: https://github.com/alexa/massive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f117e715",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(model_path, sentences_path, limit = None):\n",
    "    model = KeyedVectors.load_word2vec_format(model_path, unicode_errors = 'replace', limit = limit)\n",
    "    sentences = pd.read_json(sentences_path, lines = True)['utt']\n",
    "    \n",
    "    return model, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "553117db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths order: model path, sentences path\n",
    "fasttext_path ='datasets/'\n",
    "massive_path = 'Datasets/Amazon_Massive/'\n",
    "\n",
    "paths = {\n",
    "    'pt': [ fasttext_path + 'cc.pt.300.vec', massive_path + 'pt-PT.jsonl' ],\n",
    "    'en': [ fasttext_path + 'cc.en.300.vec', massive_path + 'en-US.jsonl' ],\n",
    "    'es': [ fasttext_path + 'cc.es.300.vec', massive_path + 'es-ES.jsonl' ]\n",
    "}\n",
    "\n",
    "languages = paths.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "08bf0f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models, sentences = {}, {}\n",
    "\n",
    "for key, value in paths.items():\n",
    "    models[key], sentences[key] = load_files(value[0], value[1], limit = 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e14fb99",
   "metadata": {},
   "source": [
    "## 2 - Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e67d11b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KeyedVectors' object has no attribute 'add'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [75]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m vectors \u001b[38;5;241m=\u001b[39m [v \u001b[38;5;28;01mfor\u001b[39;00m _,v \u001b[38;5;129;01min\u001b[39;00m s]\n\u001b[1;32m     21\u001b[0m model \u001b[38;5;241m=\u001b[39m KeyedVectors(\u001b[38;5;28mlen\u001b[39m(vectors))\n\u001b[0;32m---> 22\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m(phrases, vectors)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyedVectors' object has no attribute 'add'"
     ]
    }
   ],
   "source": [
    "samples = { key: [] for key in languages }\n",
    "\n",
    "for idx in range(len(sentences['pt'])):\n",
    "    actual_sentence = { key: [] for key in languages}\n",
    "    \n",
    "    try:\n",
    "        for lang, sent in sentences.items():\n",
    "            for word in sent[idx].split(' '):\n",
    "                actual_sentence[lang].append(models[lang][word])\n",
    "\n",
    "    except KeyError:\n",
    "        continue\n",
    "    \n",
    "    for key, value in actual_sentence.items():\n",
    "        samples[key].append([sentences[key][idx], sum(value)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e5d43f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 16521 -> Model pt samples: 9685 (58.62%)\n",
      "Total sentences: 16521 -> Model en samples: 9685 (58.62%)\n",
      "Total sentences: 16521 -> Model es samples: 9685 (58.62%)\n"
     ]
    }
   ],
   "source": [
    "size_samples = 0\n",
    "for key in sentences:\n",
    "    size_samples = len(samples[key])\n",
    "    print(f'Total sentences: { len(sentences[key]) } -> Model { key } samples: { len(samples[key]) } ({ len(samples[key]) / len(sentences[key]) * 100:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a26f729d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split 70/30\n",
    "split = int(size_samples*0.7)\n",
    "\n",
    "train_set = { key: samples[key][:split] for key in languages }\n",
    "test_set = { key: samples[key][split:] for key in languages }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf7d5e4",
   "metadata": {},
   "source": [
    "## 3 - Making the translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "df5bc3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "translations = { key: { lang: None for lang in languages if lang != key } for key in languages }\n",
    "\n",
    "# It is possible to use combinations and transpose translator to speedup and similar results\n",
    "for origin, target in it.permutations(languages, 2):\n",
    "    samples_origin = [s[1] for s in train_set[origin]]\n",
    "    samples_target = [s[1] for s in train_set[target]]\n",
    "    \n",
    "    U, Sig, Vt = np.linalg.svd(np.transpose(samples_origin) @ samples_target)\n",
    "    translator = np.transpose(Vt) @ np.transpose(U)\n",
    "    translations[origin][target] = translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ed70d7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_word_list = [\n",
    "    'sapato',\n",
    "    'flor',\n",
    "    'aniversário',\n",
    "    'saudades'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "04131c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('zapato', 0.6491979956626892), ('zapatos', 0.5523584485054016), ('bolso', 0.5190415978431702), ('calzado', 0.5010228157043457), ('calcetín', 0.4991340935230255), ('vestido', 0.4952298104763031), ('sandalias', 0.4918721914291382), ('tacones', 0.4841281771659851), ('tacón', 0.4792153835296631), ('abrigo', 0.47542041540145874)]\n",
      "===\n",
      "[('flor', 0.5486701130867004), ('flores', 0.4406846761703491), ('rosa', 0.4260789453983307), ('amapola', 0.41856929659843445), ('preciosa', 0.39803820848464966), ('maceta', 0.3977915346622467), ('guirnalda', 0.3962261378765106), ('amapolas', 0.39263394474983215), ('lavanda', 0.3908427357673645), ('hoja', 0.3826061189174652)]\n",
      "===\n",
      "[('cumpleaños', 0.7168344259262085), ('aniversario', 0.568382978439331), ('Cumpleaños', 0.5552074313163757), ('aniversarios', 0.4919818341732025), ('celebrar', 0.49020811915397644), ('festejar', 0.4850721061229706), ('celebracion', 0.4834299385547638), ('celebración', 0.48271429538726807), ('fiesta', 0.48007288575172424), ('festejo', 0.47621044516563416)]\n",
      "===\n",
      "[('ganas', 0.46721336245536804), ('nostalgias', 0.45376962423324585), ('tardes', 0.45307591557502747), ('ganitas', 0.447713166475296), ('añoranza', 0.44744592905044556), ('nostalgia', 0.44009044766426086), ('añoro', 0.43742993474006653), ('entrañables', 0.4037201702594757), ('navidades', 0.4000043272972107), ('queridas', 0.3985578417778015)]\n",
      "===\n"
     ]
    }
   ],
   "source": [
    "for pt_word in pt_word_list:\n",
    "    print(models['es'].most_similar(translations['pt']['es'] @ models['pt'][pt_word]))\n",
    "    print(\"===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eca8443",
   "metadata": {},
   "source": [
    "## 4-Evaluate\n",
    "Uses the amazon dataset to evaluate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4716de32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cossine_similarity(v1, v2):\n",
    "    return np.dot(v1, v2)/(np.linalg.norm(v1)*np.linalg.norm(v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "18a45fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adere a isto -> join this\n",
      "Cossine similarity: 0.5027318\n"
     ]
    }
   ],
   "source": [
    "#idea behind: \"translating\" the vector that one phrase represents to other should result in a similar phrase\n",
    "explanation_idx = 0\n",
    "print(test_set['pt'][explanation_idx][0], '->', test_set['en'][explanation_idx][0])\n",
    "\n",
    "vector_translated = translations['pt']['en'] @ test_set['pt'][explanation_idx][1]\n",
    "vector_target = test_set['en'][explanation_idx][1]\n",
    "\n",
    "print(\"Cossine similarity:\", cossine_similarity(vector_translated, vector_target)) # -1 to 1 interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "59e736e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avaliate_path(path):\n",
    "    translation_matrix = np.identity(300) #replace to get size from actual translation matrix\n",
    "    \n",
    "    for (origin, target) in it.pairwise(path): #accumulate matrix\n",
    "        translation_matrix = translations[origin][target] @ translation_matrix\n",
    "    \n",
    "    vectors = [translation_matrix @ v for _, v in test_set[path[0]] ]\n",
    "    vectors_target = [ v for _, v in test_set[path[-1]]]\n",
    "    \n",
    "    return sum( [ cossine_similarity(v1, v2) for v1, v2 in zip(vectors, vectors_target)])/len(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "11d4c3ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7732642441362156"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bulk test, mean, pt to es\n",
    "avaliate_path(['pt', 'es'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "21391132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7448056987423631"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bulk test, mean, pt to en to es\n",
    "avaliate_path(['pt', 'en', 'es'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246e534a",
   "metadata": {},
   "source": [
    "## 5-Other ways to evaluate *future work\n",
    "Two ways to evaluate a path between two languages using words, not phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1c08d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shoes\n",
      "[('zapatos', 0.6464644074440002), ('zapatillas', 0.6166307926177979), ('sandalias', 0.5769971013069153), ('botas', 0.5539029240608215), ('calzado', 0.551113486289978), ('zapato', 0.5218526721000671), ('chanclas', 0.5212720036506653), ('calzarán', 0.5174567103385925), ('chancletas', 0.5101444721221924), ('calcetines', 0.5067110657691956)]\n",
      "===\n",
      "flower\n",
      "[('flor', 0.5974904894828796), ('peonía', 0.5804789066314697), ('flores', 0.5449814796447754), ('peonías', 0.5210880637168884), ('floral', 0.49466392397880554), ('anturio', 0.4922579824924469), ('crisantemo', 0.4872678816318512), ('camelia', 0.47671687602996826), ('flores.La', 0.47307777404785156), ('gerbera', 0.4728681743144989)]\n",
      "===\n",
      "birthday\n",
      "[('cumpleaños', 0.7605622410774231), ('cumpleaño', 0.6504004597663879), ('cumpleños', 0.5819917321205139), ('cumpleaños.El', 0.5789167284965515), ('Cumpleaños', 0.5748640894889832), ('cumpeaños', 0.572516918182373), ('cumpleañero', 0.5690513849258423), ('cumpleaños.Y', 0.5607236623764038), ('cumpleaños.En', 0.5477907657623291), ('cumpleñaos', 0.5442656874656677)]\n",
      "===\n",
      "memories.So\n",
      "[('recuerdos', 0.5069612264633179), ('recuerdos.Y', 0.45639243721961975), ('recuerdos.En', 0.44077301025390625), ('añoranzas', 0.4395352303981781), ('Gratos', 0.4357491135597229), ('recuerdos.La', 0.4351595938205719), ('recuerdos.Un', 0.4291515052318573), ('memoria.Y', 0.4289899170398712), ('recuerdos.El', 0.4282625615596771), ('nostalgias', 0.4201315939426422)]\n",
      "===\n"
     ]
    }
   ],
   "source": [
    "# Getting the most similar word in each language it pass\n",
    "# Most expensive (uses most_similar multiple times) and try to aproximate a word each time\n",
    "for pt_word in pt_word_list:\n",
    "    english_word = models['en'].most_similar(translations['pt']['en'] @models['pt'][pt_word])[0][0]\n",
    "    print(english_word)\n",
    "    print(models['es'].most_similar(translations['en']['es'] @ models['en'][english_word]))\n",
    "    print(\"===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db82fc41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('zapato', 0.5569401383399963), ('zapatos', 0.5283559560775757), ('tacones', 0.49460408091545105), ('vestido', 0.4847036302089691), ('sapato', 0.4791000187397003), ('sapatos', 0.46534422039985657), ('pantalón', 0.46260935068130493), ('collarcito', 0.4543265402317047), ('tacón', 0.4519156813621521), ('pantalones', 0.45182451605796814)]\n",
      "===\n",
      "[('flor', 0.5727273225784302), ('peonia', 0.498028963804245), ('peonía', 0.4919746220111847), ('florecita', 0.48463183641433716), ('rosa', 0.4773949682712555), ('florecilla', 0.4752405881881714), ('flor.Y', 0.4716757535934448), ('rosaY', 0.4682154655456543), ('rosaa', 0.46463143825531006), ('azalea', 0.4611826539039612)]\n",
      "===\n",
      "[('cumpleaños', 0.6476301550865173), ('cumpleaño', 0.6001196503639221), ('cumpleños', 0.5466558933258057), ('cumpleaños.El', 0.5311540961265564), ('cumpleaños.En', 0.5147355198860168), ('cumpeaños', 0.4955201745033264), ('aniversario', 0.4934757947921753), ('cumpleanos', 0.48804065585136414), ('cumpleañero', 0.4873219132423401), ('cumpleñaos', 0.4855706989765167)]\n",
      "===\n",
      "[('morriñas', 0.47367650270462036), ('añorar', 0.4645899534225464), ('añoro', 0.4526515007019043), ('extrañamos', 0.4451638460159302), ('añoranza', 0.44479629397392273), ('nostálgia', 0.4394629895687103), ('fatiguitas', 0.4392240345478058), ('nostalgia', 0.43839192390441895), ('añoranzas', 0.4283502995967865), ('añoraré', 0.4271792471408844)]\n",
      "===\n"
     ]
    }
   ],
   "source": [
    "# Using the vector transformed to each subspace\n",
    "# Uses most_similar and try to approximate the word just one time \n",
    "for pt_word in pt_word_list:\n",
    "    english_vector = translations['pt']['en'] @models['pt'][pt_word]\n",
    "    print(models['es'].most_similar(translations['en']['es'] @ english_vector))\n",
    "    print(\"===\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cf92aa13fedf815d5c8dd192b8d835913fde3e8bc926b2a0ad6cc74ef2ba3ca2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
